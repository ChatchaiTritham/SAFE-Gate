%% Expert Systems with Applications - Elsevier
%% SAFE-Gate: Expert System for Clinical Triage Safety
%% Adapted from IEEE EMBC conference paper

\documentclass[review,12pt]{elsarticle}

%% Packages
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{lineno}

%% Line numbers for review
\modulolinenumbers[5]
\linenumbers

%% Custom commands
\newcommand{\safegate}{\textsc{SAFE-Gate}}

%% Journal name
\journal{Expert Systems with Applications}

\begin{document}

\begin{frontmatter}

%% Title
\title{SAFE-Gate: A Knowledge-Based Expert System for Emergency Triage Safety with Conservative Multi-Gate Architecture and Explainable Reasoning}

%% Authors
\author[inst1]{Chatchai Tritham\fnref{fn1}}
\ead{chatchait@nu.ac.th}
\fntext[fn1]{ORCID: 0000-0001-7899-228X}

\author[inst1]{Chakkrit Snae Namahoot\corref{cor1}\fnref{fn2}}
\ead{chakkrits@nu.ac.th}
\fntext[fn2]{ORCID: 0000-0003-4660-4590}

\cortext[cor1]{*Corresponding author: Chakkrit Snae Namahoot, Email: chakkrits@nu.ac.th}

\address[inst1]{Department of Computer Science and Information Technology, Faculty of Science, Naresuan University, Phitsanulok 65000, Thailand}

%% Abstract
\begin{abstract}
\textbf{Background:} Emergency triage of acute dizziness/vertigo presents a critical diagnostic challenge: most cases are benign, but a small percentage represent life-threatening strokes. Ensemble averaging of AI models can create dangerous blind spots when one model correctly identifies a critical case but gets outvoted by others.

\textbf{Objective:} We developed a knowledge-based expert system with formal safety guarantees for emergency triage, prioritizing critical case detection over overall accuracy.

\textbf{Methods:} SAFE-Gate employs six parallel knowledge modules examining orthogonal safety dimensions: critical red flags (G1), cardiovascular risk (G2), data quality (G3), syndrome matching (G4), uncertainty quantification (G5), and temporal analysis (G6). Instead of averaging, conservative merging selects the most cautious assessment—any module signaling high risk takes precedence. We validated six mathematical safety properties across 6,400 synthetic vertigo cases generated through counterfactual reasoning.

\textbf{Results:} On 804 held-out cases, SAFE-Gate achieved 100\% critical sensitivity (175/175 cases) with zero false negatives and perfect safe-discharge precision. Standard ensemble averaging catastrophically failed at 71.4\% sensitivity, missing 50 critical cases. Conservative architecture produces deliberate over-triage (21.3\% escalations) while maintaining real-time performance (<2ms per decision).

\textbf{Conclusions:} Conservative knowledge integration eliminates ensemble averaging's safety signal dilution through provably safe architecture. This approach provides a generalizable template for safety-critical medical expert systems where missing dangerous cases causes greater harm than unnecessary caution. Clinical deployment requires prospective validation on real patient data.
\end{abstract}

%% Keywords
\begin{keyword}
Expert systems \sep Clinical decision support \sep Safety-critical systems \sep Emergency triage \sep Explainable AI \sep Conservative reasoning
\end{keyword}

\end{frontmatter}

%% Main text
\section{Introduction}
\label{sec:introduction}

\subsection{Expert Systems in Safety-Critical Medical Decision Making}

Knowledge-based expert systems have been supporting clinical decisions since the 1970s, when MYCIN helped diagnose bacterial infections \cite{shortliffe1976mycin} and INTERNIST tackled general internal medicine problems \cite{miller1982internist}. What made these systems valuable was their transparency—physicians could see exactly which rules fired and why a particular recommendation emerged. Clinical guidelines could be encoded once and then applied consistently across thousands of cases \cite{jackson1998introduction}. But despite these advantages, expert systems never achieved the widespread adoption that many predicted.

Part of the problem stems from what researchers call the \textit{knowledge acquisition bottleneck}. Building a comprehensive expert system requires knowledge engineers to work closely with medical specialists for months or even years, painstakingly translating clinical expertise into formal logical rules \cite{feigenbaum1977art}. Medical knowledge doesn't stand still either—as new trials complete and guidelines update, someone needs to maintain and revise these knowledge bases \cite{peleg2013computer}. Perhaps more problematic, rule-based systems tend to be brittle. When a patient presents with an unusual combination of symptoms not anticipated by the rule authors, the system may fail entirely rather than degrade gracefully \cite{davis1993knowledge}.

Machine learning seemed to offer a way around these limitations. Rather than hand-crafting rules, modern deep learning systems can extract patterns from massive electronic health record databases \cite{rajkomar2018scalable}. They adapt naturally to atypical cases and don't require explicit programming for every possible scenario. But this flexibility comes at a cost. The resulting models often function as ``black boxes'' that resist clinical interpretation \cite{tonekaboni2019clinicians}, making it difficult for physicians to validate their reasoning or identify potential errors. Without formal safety guarantees, these systems can fail unpredictably on rare edge cases \cite{challen2019artificial}. Privacy regulations also limit access to the large training datasets these approaches require \cite{price2019privacy}. Recently, researchers have begun exploring hybrid architectures that combine symbolic reasoning with neural learning \cite{pearl2018theoretical,bengio2019meta}, but these systems remain largely in the laboratory.

Emergency triage for acute dizziness and vertigo illustrates why we need better approaches. These complaints account for 3--4\% of emergency department visits \cite{newman2008spectrum}, and they pose a genuine diagnostic puzzle. The vast majority of cases—roughly 40 out of every 41—turn out to be benign inner ear problems. But that one remaining case might be a posterior circulation stroke that could leave the patient severely disabled or dead if missed during the narrow window for intervention. The challenge is that benign and dangerous causes can look remarkably similar initially. Only 3--5\% of patients with acute vestibular symptoms ultimately receive stroke diagnoses \cite{kattah2009hints}, yet about one in five posterior strokes presents without the classic warning signs that emergency physicians have been trained to recognize \cite{tarnutzer2011bedside}. This creates an impossible dilemma: refer everyone for neuroimaging and you waste vast resources while overwhelming radiology departments; try to be selective and you risk missing strokes when patients are still within the thrombolysis window.

\subsection{The Ensemble Averaging Paradox in Safety-Critical Systems}

Over the past two decades, ensemble methods have emerged as one of machine learning's most successful ideas. The basic premise is elegant: combine predictions from multiple models, and their independent errors will tend to cancel out \cite{dietterich2000ensemble}. In practice, this usually means averaging predicted probabilities or taking majority votes across different classifiers. For optimizing overall accuracy on balanced datasets, this approach works remarkably well.

But there's a subtle problem when we deploy these techniques in safety-critical medical applications. Imagine a patient presenting with symptoms that could indicate either a benign condition or a life-threatening emergency. One of your ensemble members—perhaps a model that specializes in identifying high-risk features—correctly recognizes this as a critical case and outputs a high-risk prediction. However, four other models in your ensemble, looking at different feature perspectives, classify it as moderate risk. When you average these five predictions together, the mathematics inevitably pulls the result toward the majority view: moderate risk. The critical warning from that one cautious model gets diluted away. If that conservative model was actually right, you've just created a false negative on exactly the kind of case you most need to catch.

This problem gets worse as class imbalance increases. In emergency triage for dizziness, critical cases make up perhaps 5\% of presentations. Standard statistical optimization naturally pushes predictions toward the 95% majority—the benign cases. The ensemble learns that predicting "moderate risk" is usually correct, so conservative warnings from individual models get systematically outweighed.

Some researchers have tried to address this through selective prediction or abstention \cite{geifman2017selective}—having the system refuse to make predictions when it's uncertain. But this creates its own difficulties. How uncertain is too uncertain? Set the abstention threshold too cautiously and you end up deferring 90\% or more of cases back to human clinicians, which defeats the entire purpose of automation. Set it too permissively and you're back to missing critical cases. Clinical pilots have struggled to find any middle ground. The confidence-based approach also breaks down when neural networks exhibit the overconfidence problem—assigning high probability scores to incorrect predictions on unusual cases \cite{guo2017calibration}.

\subsection{Conservative Knowledge Integration: A Fail-Safe Architecture}

What if we inverted the usual approach to combining multiple models? Instead of averaging their outputs to maximize overall accuracy, what if we merged them conservatively—always selecting the most cautious assessment? This shifts the design goal from statistical optimization to safety optimization.

That's the core idea behind \safegate{}. The system evaluates each patient through six independent knowledge modules running in parallel, with each module examining a different dimension of risk. The first module (G1) applies hard-coded rules straight from emergency medicine guidelines, checking for critical red flags like unstable vital signs or acute neurological deficits. The second (G2) accumulates cardiovascular risk factors—age, hypertension, diabetes, atrial fibrillation—in a weighted scoring approach similar to established clinical instruments. The third module (G3) serves a quality control function, monitoring whether we have enough clinical information to make a reliable automated assessment or whether we should defer to a human clinician. The fourth (G4) tries to match the symptom pattern against well-characterized benign syndromes using the TiTrATE diagnostic framework \cite{kattah2009hints}—if a patient fits a classic benign profile, that provides reassurance. Module five (G5) takes a different approach, using a Bayesian neural network with Monte Carlo dropout to estimate how confident the system should be about its prediction. The final module (G6) considers the time course—how symptoms began and how they've evolved—because temporal patterns carry crucial diagnostic information for distinguishing strokes from peripheral vestibular problems.

These six assessments feed into a conservative merging step. We define a risk ordering: abstention (defer to human) is most conservative, followed by critical (immediate intervention), high-risk (urgent evaluation), moderate (same-day assessment), low-risk (monitoring), and minimal (safe discharge). When the gates disagree, the final output always selects the most conservative assessment. If five modules say "moderate risk" but one says "critical," the system outputs critical. If any module triggers abstention because of high uncertainty or missing data, that overrides everything else.

This design provides three advantages that traditional ensemble averaging cannot match. First, we can formally prove safety properties. Statements like "no critical case will ever receive a safe discharge recommendation" or "increasing clinical severity always moves the risk tier in a more conservative direction" become mathematical theorems that can be verified computationally. We've tested six such properties across 6,400 synthetic cases without finding a single violation. Second, the system produces clear explanations—the audit trail shows exactly which module enforced the final decision and why, giving clinicians something concrete to review and verify. Third, and most fundamentally, conservative merging guarantees that safety warnings propagate through to the final output. Unlike averaging, which can dilute or suppress cautious predictions, minimum selection ensures they always have a voice.

\subsection{Contributions and Scope}

This work offers four main contributions to the expert systems literature.

\textbf{First}, we introduce and formalize a conservative knowledge integration architecture based on lattice minimum selection rather than statistical averaging. This approach provably eliminates the failure mode where ensemble averaging dilutes safety warnings. While we demonstrate this in medical triage, the principle applies broadly to any safety-critical domain where missing a dangerous case causes more harm than raising false alarms.

\textbf{Second}, we show how to effectively combine symbolic rules with statistical learning in a hybrid architecture. Four of our six modules (G1, G3, G4, G6) use explicit rule-based logic that clinicians can directly inspect and validate. The other two (G2, G5) employ data-driven pattern recognition to handle the probabilistic aspects of risk assessment and uncertainty quantification. This combination provides both the interpretability of traditional expert systems and the adaptability of modern machine learning.

\textbf{Third}, we develop a formal safety property framework with computational verification. We specify six mathematical properties the system should satisfy—things like "no critical case receives a discharge recommendation" and "increasing clinical severity always produces more conservative risk tiers"—and then verify these properties computationally across our test dataset. This provides a practical middle ground between pure statistical validation and full formal proof.

\textbf{Fourth}, we empirically validate the ensemble averaging failure hypothesis through direct comparison. Testing the same components configured as a standard averaging ensemble versus our conservative architecture on 804 cases demonstrates the dramatic difference: 71.4\% sensitivity with 50 missed critical cases for averaging, versus 100\% sensitivity with zero misses for conservative merging.

\textit{Important scope limitations:} This study establishes computational feasibility using entirely synthetic data. We generated these cases through counterfactual reasoning and non-negative matrix factorization to ensure realistic symptom patterns, which allowed rapid iteration and controlled testing without privacy concerns. However, synthetic data—no matter how carefully constructed—cannot fully capture the complexity, variability, and messiness of real emergency department presentations. Before any clinical deployment, this system requires multiple validation steps. Emergency physicians and neurologists need to review the encoded clinical logic to verify we've correctly interpreted guidelines and haven't introduced errors. The system needs retrospective testing on actual de-identified patient records to see how it performs on real cases. Prospective observational deployment should measure how often clinicians agree with or override the system's recommendations. Ultimately, randomized controlled trials comparing patient outcomes—stroke detection rates, time to thrombolysis, functional outcomes at 90 days, cost-effectiveness—would be needed to determine if the system actually improves care. The architecture is specifically designed for domains where conservative bias makes clinical sense, where missing a critical diagnosis causes far more harm than unnecessary escalation \cite{powers2019ahastroke}.

The remainder of this paper proceeds as follows: Section \ref{sec:related} reviews relevant work in medical expert systems, ensemble learning methods, and formal verification approaches; Section \ref{sec:methods} describes the six-module architecture in detail, explains the conservative merging algorithm, and outlines our synthetic data generation process; Section \ref{sec:results} presents performance metrics on 804 held-out test cases with comparisons to baseline methods; Section \ref{sec:discussion} examines implications, limitations, and requirements for clinical validation; Section \ref{sec:conclusion} summarizes our findings and charts the path toward prospective evaluation.

\section{Related Work}
\label{sec:related}

\subsection{Expert Systems for Medical Decision Support}

Looking back at the history of medical expert systems reveals three fairly distinct waves of development. The earliest systems from the 1970s and early 1980s—MYCIN for identifying bacterial infections \cite{shortliffe1976mycin} and INTERNIST for broader internal medicine problems \cite{miller1982internist}—relied entirely on hand-coded rules combined with certainty factors. They worked remarkably well within their narrow domains but ran into two persistent problems. First, building them required enormous effort; MYCIN's 600 rules represented several person-years of painstaking work with domain experts \cite{feigenbaum1977art}. Second, they tended to fail completely when encountering cases that didn't fit their programmed rules—the brittleness problem we mentioned earlier.

The second generation tried to address these limitations by incorporating probabilistic reasoning. Systems built around Bayesian networks \cite{pearl1988probabilistic} or fuzzy logic \cite{zadeh1996fuzzy} could handle uncertainty and incomplete information more gracefully. Applications emerged for diagnosing heart disease \cite{haddawy1994clinical}, pneumonia \cite{shwe1991probabilistic}, and cancer staging \cite{spiegelhalter1993sequential}. But while these systems dealt better with uncertainty, they didn't really solve the knowledge acquisition problem. Someone still needed to manually specify all those conditional probability tables or fuzzy membership functions.

More recently, a third generation has emerged that tries to get the best of both worlds by combining symbolic knowledge with machine learning. The idea is to automatically extract some knowledge from clinical databases while keeping the overall structure interpretable. Systems like DXplain \cite{barnett2008dxplain}, Isabel \cite{ramnarayan2003isabel}, and VisualDx \cite{freiman2017visualdx} represent this hybrid approach. Yet despite these advances, emergency departments have been slow to adopt these tools. Clinicians worry about the "black box" aspects of the learned components \cite{tonekaboni2019clinicians}, lack confidence in their safety for time-critical decisions \cite{challen2019artificial}, and struggle to integrate them into existing workflows \cite{sendak2020presenting}.

Our work builds on this hybrid tradition but adds two novel elements. First, we formalize how to combine multiple knowledge sources conservatively rather than through averaging, eliminating a specific failure mode that plagues conventional ensembles. Second, we demonstrate how to specify and computationally verify safety properties in a way that could support regulatory review of safety-critical medical AI systems.

\subsection{Ensemble Methods and Their Limitations}

Ensemble learning has become one of the field's most reliable techniques for boosting predictive performance. The core idea is simple: combine multiple models and let their independent mistakes cancel out \cite{dietterich2000ensemble}. In practice, people typically use one of three combination strategies. Averaging computes a weighted sum or mean of predicted probabilities \cite{breiman1996bagging}. Voting takes the majority opinion across classifiers \cite{freund1996experiments}. Stacking trains a meta-model to learn how to optimally combine the base predictions \cite{wolpert1992stacked}.

There's solid theory behind why ensembles work. When individual models make independent errors, combining them reduces variance and improves generalization \cite{krogh1995neural,kuncheva2003measures}. Practical successes abound: random forests eliminate much of the overfitting that plagues individual decision trees \cite{breiman2001random}, gradient boosting iteratively fixes residual errors to achieve state-of-the-art performance \cite{friedman2001greedy}, and deep ensembles improve the calibration of neural network predictions \cite{lakshminarayanan2017simple}.

But here's the problem we keep coming back to. When you apply ensemble averaging in a safety-critical setting with severe class imbalance—say, critical cases making up less than 5% of your data—the statistical optimization naturally gravitates toward the majority class. Picture our earlier scenario: one model correctly flags a case as high risk (R1), but four others call it moderate (R3). Take the average and you get...moderate. The safety signal vanishes. The more imbalanced your classes, the worse this problem becomes.

Researchers have tried various fixes. Selective prediction allows models to abstain when they're uncertain \cite{geifman2017selective}. Epistemic uncertainty quantification tries to estimate when a model is operating outside its comfort zone \cite{gal2016dropout}. But both approaches struggle with threshold selection. Set your abstention threshold too conservatively and the system refuses to make predictions for 90% of cases, which makes it useless. Set it too permissively and you're back to missing critical warnings. Our conservative merging framework sidesteps this entire problem. Instead of trying to statistically calibrate an abstention threshold, we enforce safety through a simple rule: any module that signals critical risk automatically overrides more optimistic assessments.

\subsection{Clinical Triage Decision Support Systems}

Emergency departments have long relied on structured triage tools to help sort incoming patients by urgency. Traditional approaches use validated scoring systems that have been refined over years of clinical use. The Emergency Severity Index (ESI) \cite{gilboy2011emergency} sorts patients into five urgency levels. The Modified Early Warning Score (MEWS) \cite{subbe2001validation} tries to predict which patients might deteriorate during their hospital stay. The National Early Warning Score (NEWS2) \cite{royal2017national} provides standardized criteria for identifying acutely ill patients. These rule-based tools offer excellent inter-rater reliability—different nurses using the same tool tend to reach the same conclusion. Their main limitation is inflexibility when patients present with complicated combinations of symptoms and comorbidities that don't quite fit the standard patterns.

Machine learning promised to overcome this limitation by learning patterns directly from data. Researchers have developed neural networks for predicting sepsis from electronic health records \cite{komorowski2018artificial}, gradient boosting models for stratifying acute coronary syndrome risk \cite{shouval2017machine}, and natural language processing systems for detecting strokes from clinical notes \cite{bacchi2020machine}. The discrimination metrics often look impressive in development. But real-world deployment has been sobering. One widely-implemented sepsis alert system generated false alarms 67% of the time, leading to alert fatigue and eroding clinician trust \cite{wong2021epic}. Post-hoc explainability methods like SHAP and LIME can identify which features mattered most to a prediction, but physicians report that these explanations don't really help them understand or validate the system's reasoning \cite{tonekaboni2019clinicians}.

For vestibular disorders specifically, clinicians have several established tools. The HINTS protocol (Head Impulse, Nystagmus, Test of Skew) helps differentiate strokes from peripheral causes \cite{kattah2009hints}. The ABCD² score stratifies TIA risk \cite{johnston2007validation}. The TiTrATE framework formalizes diagnostic criteria along temporal and phenotypic dimensions \cite{newman2015timing}. When applied correctly by skilled examiners, these tools achieve excellent sensitivity—typically above 95%. The catch is that some components require specialized examination skills, like proper interpretation of the head impulse test, that not all emergency providers possess.

\safegate{} attempts to bridge these worlds. The symbolic modules (G1 and G4) encode established clinical rules and decision criteria, providing the reliability and transparency of traditional tools. The statistical modules (G2 and G5) learn probabilistic patterns from data, offering the adaptability of modern machine learning. Critically, the conservative merging ensures that hard safety constraints—like "unstable vital signs require immediate escalation"—can never be overridden by statistical predictions that might have learned to be too permissive. This addresses what we see as a crucial gap in existing hybrid systems.

\subsection{Formal Verification for Safety-Critical AI}

Industries with genuinely high-stakes systems—aviation, nuclear power, medical devices—don't just test their products and hope for the best. They use formal verification methods to mathematically prove that systems satisfy critical safety properties \cite{knight2002safety}. Model checking exhaustively explores all possible states a system could enter \cite{clarke1999model}. Theorem proving establishes logical correctness through rigorous proof \cite{nipkow2002isabelle}. Abstract interpretation analyzes program behavior without executing every possible path \cite{cousot1977abstract}.

Researchers have recently begun adapting these techniques for machine learning. Reluplex can verify certain robustness properties of neural networks by solving satisfiability problems \cite{katz2017reluplex}. Certified robustness methods provide provable bounds on how much adversarial perturbation a network can tolerate \cite{cohen2019certified}. Bayesian approaches attempt to quantify uncertainty in learned components \cite{katz2020verify}. However, most of this work focuses on continuous perturbations to image classifiers—how much can you perturb pixel values before the network changes its answer—rather than discrete clinical logic.

Meanwhile, regulators are starting to demand more rigor for medical AI. The FDA's Good Machine Learning Practice principles \cite{fda2021gmlp} call for healthcare-grade performance with documented safety margins, transparent decision-making that clinicians can understand, and demonstrated robustness when the data distribution shifts. The European Union's Medical Device Regulation \cite{eu2017mdr} requires formal risk management and safety analysis throughout a product's lifecycle.

Our approach aims for a practical middle ground. Full formal verification of systems with complex machine learning components remains computationally prohibitive. But pure statistical validation—reporting accuracy on a test set—provides no guarantees about safety properties. We specify six safety properties in mathematical form (for example: "for all cases x, if x is critical, then the output must be R1, R2, or abstention—never a discharge recommendation"). Then we check these properties computationally across our entire dataset. We found zero violations across 6,400 synthetic cases. This doesn't constitute an exhaustive mathematical proof, but it provides much stronger evidence than standard test set metrics, potentially sufficient for regulatory submissions.

\section{Methods}
\label{sec:methods}

\subsection{Expert System Architecture and Knowledge Representation}

The \safegate{} system processes each patient through three computational stages. First, six knowledge modules run in parallel, each evaluating a different safety dimension independently. Second, a conservative merging step combines these assessments by selecting the most cautious one. Third, the system generates an audit trail explaining which module drove the final decision and why. The architecture comprises:

\textbf{Stage 1 - Parallel Gate Evaluation:} Six independent knowledge modules (G1--G6) simultaneously assess patient data from orthogonal safety perspectives: G1 employs rule-based logic for critical red flag detection, G2 performs cardiovascular risk scoring through weighted factor accumulation, G3 evaluates data completeness and quality, G4 matches symptom patterns against established benign clinical syndromes, G5 quantifies epistemic uncertainty via Bayesian neural networks with Monte Carlo dropout, and G6 analyzes temporal symptom evolution patterns.

\textbf{Stage 2 - Conservative Merging:} Rather than averaging gate outputs like traditional ensembles, the system selects the lattice minimum (most conservative assessment) across all six modules, ensuring that any single gate signaling critical risk or abstention overrides less conservative assessments from other modules.

\textbf{Stage 3 - Audit Trail Generation:} The system produces an explainable decision trace documenting which specific gate enforced the final risk tier, what clinical features triggered that assessment, and the underlying rationale—supporting physician review and quality assurance.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/architecture.pdf}
\caption{\safegate{} expert system architecture showing the three-stage pipeline. Patient data flows through six parallel knowledge modules (G1--G6), each evaluating orthogonal safety dimensions: G1 (critical red flag detection via rules), G2 (cardiovascular risk scoring), G3 (data quality assessment), G4 (clinical syndrome pattern matching), G5 (epistemic uncertainty quantification via Bayesian neural network), and G6 (temporal risk analysis). Conservative merging selects the lattice minimum across all gate outputs, ensuring that any module signaling critical risk or abstention overrides less conservative assessments. The system generates an explainable audit trail documenting which gate enforced the final tier and the clinical rationale.}
\label{fig:architecture}
\end{figure}

\textbf{Knowledge representation formalism:} Formally, we're mapping from a patient state space $\mathcal{X}$ (containing all clinical features) to a risk tier space $\mathcal{R}$ with a specific ordering:
\begin{equation}
\mathcal{R} = \{R^*, R1, R2, R3, R4, R5\}, \quad R^* \sqsubseteq R1 \sqsubseteq R2 \sqsubseteq R3 \sqsubseteq R4 \sqsubseteq R5
\end{equation}
Here, $R^*$ means abstention—the system recognizes its uncertainty exceeds safe bounds and defers to human judgment. $R1$ represents critical conditions requiring intervention within 5 minutes. $R2$ indicates high-risk situations needing urgent evaluation within 15 minutes. $R3$ signifies moderate risk warranting same-day assessment within 30 to 120 minutes. $R4$ suggests low-risk cases suitable for monitoring over 1 to 4 hours. $R5$ corresponds to minimal risk where safe discharge with outpatient follow-up is appropriate. The partial order $\sqsubseteq$ (read as "at least as conservative as") captures our key design principle: earlier tiers in this ordering represent greater caution.

Each gate implements knowledge module $g_i: \mathcal{X} \rightarrow \mathcal{R}$ mapping patient presentations to risk tiers based on specialized expertise domain. The six gates encode complementary knowledge:

\begin{equation}
\begin{aligned}
g_1(x) &: \text{Critical red flag detection (rule-based)} \\
g_2(x) &: \text{Cardiovascular risk assessment (statistical)} \\
g_3(x) &: \text{Data quality evaluation (completeness)} \\
g_4(x) &: \text{Clinical syndrome matching (rule-based)} \\
g_5(x) &: \text{Epistemic uncertainty quantification (Bayesian)} \\
g_6(x) &: \text{Temporal risk analysis (state machine)}
\end{aligned}
\end{equation}

Conservative knowledge integration computes final assessment as lattice infimum (greatest lower bound):
\begin{equation}
r_{\text{final}}(x) = \inf_{\sqsubseteq}\{g_1(x), g_2(x), g_3(x), g_4(x), g_5(x), g_6(x)\}
\end{equation}

Since our risk lattice is totally ordered (chain), this reduces to minimum selection in practice. Critically, any gate triggering $R^*$ (abstention) overrides all other assessments by lattice precedence; any gate signaling $R1$ (critical) blocks lower-risk outputs.

\subsection{Gate 1: Critical Red Flag Detection (Rule-Based Knowledge)}

Gate G1 implements deterministic safety screening through 18 hard-coded rules encoding emergency medicine clinical guidelines \cite{powers2019ahastroke}. Knowledge representation uses production rules: IF (condition) THEN (risk tier). Table \ref{tab:g1_rules} shows critical flag definitions.

\begin{table}[htbp]
\centering
\caption{Gate 1 critical red flag rules. Rule-based knowledge encoding emergency medicine guidelines for life-threatening symptoms requiring immediate intervention. Any positive finding triggers R1 output; absence of all flags yields R5. Rules provide deterministic fail-safe guaranteeing critical case detection when symptom patterns match encoded knowledge.}
\label{tab:g1_rules}
\small
\begin{tabular}{p{0.3\textwidth}p{0.15\textwidth}p{0.35\textwidth}}
\toprule
\textbf{Red Flag Category} & \textbf{Output} & \textbf{Clinical Rationale} \\
\midrule
Hemodynamic instability: SBP $<$90 or $>$180 mmHg & R1 & Hypotension/hypertensive emergency \\
Altered mental status: GCS $<$14 & R1 & Potential brainstem ischemia \\
Acute focal deficits: diplopia, dysarthria, weakness & R1 & Posterior circulation stroke \\
Severe headache: sudden thunderclap onset & R1 & Subarachnoid hemorrhage \\
Respiratory compromise: O$_2$ sat $<$92\%, RR $>$24 & R1 & Impending failure \\
\midrule
Absence of all flags & R5 & No immediate threat \\
\bottomrule
\end{tabular}
\end{table}

The gate operates as a fail-safe module: detection of \textit{any} red flag immediately outputs R1 regardless of other clinical features, implementing the safety principle ``presence of life-threatening signs mandates immediate escalation.'' Absence of all 18 flags yields R5 (minimal risk from G1 perspective), allowing other gates' assessments to dominate through conservative merging.

This rule-based approach provides three advantages: (1) \textit{Deterministic behavior}—no probabilistic thresholds or learned parameters, ensuring consistent application of clinical guidelines; (2) \textit{Explainability}—audit trail explicitly lists which specific rule fired; (3) \textit{Clinical validation}—physicians can directly verify rule correctness against established guidelines without requiring ML expertise.

\subsection{Gate 2: Cardiovascular Risk Assessment (Statistical Knowledge)}

Gate G2 integrates multiple risk factors through weighted accumulation model trained on synthetic data distribution. Knowledge representation combines domain expertise (weight initialization based on clinical risk scores like ABCD² \cite{johnston2007validation}) with data-driven refinement (gradient boosting optimizes weights for dataset-specific patterns).

Risk score computation aggregates contributions from three feature categories:
\begin{equation}
\text{Score}(x) = w_{\text{demo}} \cdot f_{\text{demo}}(x) + w_{\text{symp}} \cdot f_{\text{symp}}(x) + w_{\text{hist}} \cdot f_{\text{hist}}(x)
\end{equation}
where $f_{\text{demo}}$ extracts demographic risk factors (age $>$60 years: +1.0, male sex: +0.5), $f_{\text{symp}}$ quantifies symptom characteristics (sudden onset: +1.5, continuous duration $>$1 hour: +1.0, severe imbalance: +0.8), and $f_{\text{hist}}$ incorporates medical history (hypertension: +1.2, diabetes: +0.9, atrial fibrillation: +1.8, prior stroke/TIA: +2.0).

Score-to-tier mapping implements threshold-based classification:
\begin{equation}
g_2(x) = \begin{cases}
R2 & \text{if Score}(x) \geq 6.0 \\
R3 & \text{if } 3.0 \leq \text{Score}(x) < 6.0 \\
R4 & \text{if } 1.0 \leq \text{Score}(x) < 3.0 \\
R5 & \text{if Score}(x) < 1.0
\end{cases}
\end{equation}

A gradient-boosted decision tree (XGBoost with 100 estimators, max depth 5, learning rate 0.05) validates scoring consistency and provides supplementary probabilistic assessment. If tree prediction disagrees with score-based tier by more than one level, the more conservative assessment dominates.

This hybrid statistical approach balances interpretability (explicit weight-based scoring physicians can inspect) with adaptability (boosting captures nonlinear interactions in training data).

\subsection{Gate 3: Data Quality Assessment (Completeness Knowledge)}

Gate G3 encodes meta-knowledge about when available information suffices for reliable automated assessment versus requiring human oversight. This gate implements the epistemic principle: ``incomplete data mandates conservative escalation or abstention.''

Knowledge representation defines essential clinical field set $\mathcal{F}_{\text{ess}}$ comprising 22 features: demographics (age, sex), vital signs (blood pressure, heart rate, respiratory rate, O$_2$ saturation, temperature), neurological examination (HINTS components, gait, coordination, cranial nerve function), symptom characteristics (onset pattern, duration, triggers, exacerbating/relieving factors), and medical history (cardiovascular disease, diabetes, medications).

Data completeness metric:
\begin{equation}
\rho_{\text{comp}}(x) = \frac{|\{f \in \mathcal{F}_{\text{ess}} : f(x) \neq \text{missing}\}|}{|\mathcal{F}_{\text{ess}}|}
\end{equation}

Tier assignment based on completeness thresholds:
\begin{equation}
g_3(x) = \begin{cases}
R^* & \text{if } \rho_{\text{comp}}(x) < 0.70 \text{ (force abstention)} \\
\text{escalate 1 tier} & \text{if } 0.70 \leq \rho_{\text{comp}}(x) < 0.85 \\
\text{neutral} & \text{if } \rho_{\text{comp}}(x) \geq 0.85
\end{cases}
\end{equation}

When completeness falls below 70\%, G3 outputs R$^*$ triggering abstention through conservative merging—deferring to human clinicians who can gather additional history or perform targeted examination. Moderate incompleteness (70--85\%) escalates other gates' consensus by one tier, implementing precautionary principle. High completeness ($\geq$85\%) allows G3 to contribute neutral signal (R5), permitting other gates to dominate final assessment.

This gate addresses a critical gap in existing medical AI systems: most deployed models fail silently on incomplete inputs, producing predictions despite missing essential features \cite{sendak2020presenting}. Explicit data quality monitoring provides safety guardrail.

\subsection{Gate 4: Clinical Syndrome Pattern Matching (Rule-Based Knowledge)}

Gate G4 encodes formalized clinical decision rules from the TiTrATE (Timing, Triggers, Targeted examination) diagnostic framework \cite{newman2015timing} for vestibular disorders. Knowledge representation uses multi-criteria pattern matching against established benign syndrome profiles.

\textbf{Benign paroxysmal positional vertigo (BPPV):}
\begin{itemize}
\item \textit{Timing:} Episodic attacks $<$60 seconds duration
\item \textit{Triggers:} Specific head position changes (lying down, rolling in bed, looking up)
\item \textit{Targeted exam:} Dix-Hallpike maneuver positive (latent rotatory nystagmus)
\end{itemize}

\textbf{Vestibular neuritis:}
\begin{itemize}
\item \textit{Timing:} Continuous vertigo hours to days, gradual improvement
\item \textit{Triggers:} Spontaneous onset, not positional
\item \textit{Targeted exam:} HINTS negative for stroke (normal head impulse OR horizontal nystagmus without skew)
\end{itemize}

\textbf{Meniere's disease:}
\begin{itemize}
\item \textit{Timing:} Episodic attacks 20 minutes to 12 hours
\item \textit{Triggers:} Spontaneous, associated hearing loss and tinnitus
\item \textit{Targeted exam:} Horizontal nystagmus during attacks, normal between episodes
\end{itemize}

Pattern matching algorithm scores symptom profile similarity to each syndrome template using weighted Hamming distance. If maximum similarity exceeds threshold (0.85) and no red flags present, G4 outputs R5 (confident benign match permits safe discharge). Ambiguous patterns (0.60--0.85 similarity) default to R3 (moderate risk, requires specialist evaluation). Poor matches ($<$0.60 similarity) or atypical features escalate to R2 (high suspicion for central etiology).

This gate captures critical clinical expertise: experienced emergency physicians recognize benign peripheral syndromes through characteristic presentation patterns, avoiding unnecessary imaging. Formalizing these patterns into explicit rules enables automated application while maintaining transparency.

\subsection{Gate 5: Epistemic Uncertainty Quantification (Bayesian Knowledge)}

Gate G5 quantifies prediction confidence through Bayesian neural network with Monte Carlo (MC) dropout \cite{gal2016dropout}, providing principled uncertainty estimates that trigger abstention when model epistemic uncertainty exceeds safety thresholds.

\textbf{Architecture:} Three-layer feedforward network (input: 52 features $\rightarrow$ hidden: 128 ReLU units with dropout 0.3 $\rightarrow$ hidden: 64 ReLU units with dropout 0.3 $\rightarrow$ output: 5-class softmax for R1--R5). Training minimizes categorical cross-entropy with L2 regularization ($\lambda = 0.001$) over 100 epochs using Adam optimizer (learning rate 0.001).

\textbf{Uncertainty estimation:} At inference time, perform $T = 20$ forward passes with dropout enabled, generating probability distribution samples $\{p^{(t)}\}_{t=1}^T$. Compute:
\begin{align}
\text{Predictive entropy: } H &= -\sum_{i=1}^5 \bar{p}_i \log \bar{p}_i, \quad \bar{p}_i = \frac{1}{T}\sum_{t=1}^T p_i^{(t)} \\
\text{Prediction variance: } \sigma^2 &= \frac{1}{T}\sum_{t=1}^T \|\mathbf{p}^{(t)} - \bar{\mathbf{p}}\|^2
\end{align}

Combined uncertainty score normalizes and averages entropy and variance:
\begin{equation}
\mu(x) = 0.5 \cdot \frac{H(x)}{H_{\max}} + 0.5 \cdot \frac{\sigma(x)}{\sigma_{\max}}
\end{equation}
where $H_{\max} = \log 5$ (maximum entropy for 5 classes) and $\sigma_{\max}$ is calibrated on validation set (99th percentile).

Tier assignment based on uncertainty:
\begin{equation}
g_5(x) = \begin{cases}
R^* & \text{if } \mu(x) \geq 0.80 \text{ (high uncertainty, abstain)} \\
\text{escalate 2 tiers} & \text{if } 0.60 \leq \mu(x) < 0.80 \\
\text{escalate 1 tier} & \text{if } 0.30 \leq \mu(x) < 0.60 \\
\text{use NN prediction} & \text{if } \mu(x) < 0.30 \text{ (confident)}
\end{cases}
\end{equation}

This gate implements the safety principle: ``under high epistemic uncertainty, defer to human judgment rather than risk misclassification.'' Unlike point-estimate classifiers that output confident predictions even on out-of-distribution cases, Bayesian approaches explicitly quantify when the model lacks knowledge to make reliable assessment \cite{kendall2017uncertainties}.

\subsection{Gate 6: Temporal Risk Analysis (State Machine Knowledge)}

Gate G6 encodes time-dependent risk assessment through finite state machine modeling symptom evolution patterns. Knowledge representation captures clinical principle: ``time course provides critical diagnostic information distinguishing vascular from peripheral etiologies'' \cite{powers2019ahastroke}.

State definitions based on symptom onset and progression:
\begin{itemize}
\item \textit{Hyperacute state} ($<$1 hour onset, worsening): Stroke time window, R1
\item \textit{Acute stable} (1--24 hours, stable course): R2--R3 depending on severity
\item \textit{Acute improving} (1--24 hours, improving): R3--R4
\item \textit{Subacute} (1--7 days, gradual onset): R3--R4
\item \textit{Chronic} ($>$7 days, episodic or chronic): R4--R5
\end{itemize}

State machine transitions model symptom progression trajectories. Rapid progression (stable $\rightarrow$ worsening within hours) triggers escalation; gradual improvement (acute $\rightarrow$ improving) permits de-escalation.

Tier assignment:
\begin{equation}
g_6(x) = \begin{cases}
R1 & \text{if hyperacute AND worsening} \\
R2 & \text{if acute stable AND concerning features} \\
R3 & \text{if acute improving OR subacute} \\
R4 & \text{if chronic episodic} \\
R5 & \text{if chronic stable, improving}
\end{cases}
\end{equation}

This gate formalizes the ``time is brain'' principle from stroke guidelines: patients within thrombolysis window (4.5 hours) with concerning presentations receive higher urgency to maximize treatment effectiveness.

\subsection{Conservative Knowledge Merging Algorithm}

Algorithm \ref{alg:conservative_merge} formalizes knowledge integration through lattice minimum selection. Unlike ensemble averaging which computes $\frac{1}{6}\sum_{i=1}^6 g_i(x)$ potentially diluting conservative signals, our approach selects $\inf_{\sqsubseteq}\{g_1(x), \ldots, g_6(x)\}$ ensuring individual module warnings propagate to final output.

\begin{algorithm}
\caption{Conservative Knowledge Merging}
\label{alg:conservative_merge}
\begin{algorithmic}[1]
\REQUIRE Patient state $x \in \mathcal{X}$
\REQUIRE Gate outputs $\{r_1, r_2, r_3, r_4, r_5, r_6\}$ where $r_i \in \{R^*, R1, R2, R3, R4, R5\}$
\ENSURE Final tier $r_{\text{final}}$, enforcing gate $g_{\text{enforce}}$, audit trail $\mathcal{A}$
\STATE Initialize audit trail: $\mathcal{A} \gets \{\}$
\FOR{$i = 1$ to $6$}
    \STATE Append to $\mathcal{A}$: ``Gate $G_i$ output: $r_i$, confidence: $c_i$''
\ENDFOR
\IF{$R^* \in \{r_1, \ldots, r_6\}$} \COMMENT{Abstention-first priority}
    \STATE $g_{\text{enforce}} \gets \arg\min_i \{i : r_i = R^*\}$ \COMMENT{First gate triggering abstention}
    \STATE Append to $\mathcal{A}$: ``Abstention enforced by Gate $g_{\text{enforce}}$''
    \RETURN $r_{\text{final}} = R^*$, $g_{\text{enforce}}$, $\mathcal{A}$
\ENDIF
\STATE \COMMENT{Most-conservative selection on lattice}
\STATE Define tier ranks: $\text{rank}(R1)=1, \text{rank}(R2)=2, \ldots, \text{rank}(R5)=5$
\STATE $r_{\text{final}} \gets r_i$ where $i = \arg\min_j \text{rank}(r_j)$ \COMMENT{Minimum rank = most conservative}
\STATE $g_{\text{enforce}} \gets i$
\STATE Append to $\mathcal{A}$: ``Tier $r_{\text{final}}$ enforced by Gate $g_{\text{enforce}}$''
\RETURN $r_{\text{final}}$, $g_{\text{enforce}}$, $\mathcal{A}$
\end{algorithmic}
\end{algorithm}

The algorithm implements two-stage merging: (1) lines 4--8 enforce abstention-first priority—if any gate signals $R^*$, human judgment overrides all other assessments; (2) lines 10--13 select most conservative tier among definitive assessments (lowest rank on lattice). This design ensures safety properties:

\textbf{Property 1 (Conservative preservation):} The merging function satisfies $r_{\text{final}} \sqsubseteq r_i$ for all gate outputs $r_i$. \textit{Proof:} By construction, $r_{\text{final}} = \arg\min_{\sqsubseteq}\{r_1, \ldots, r_6\}$, hence $r_{\text{final}}$ is a lower bound in lattice $(\mathcal{R}, \sqsubseteq)$. $\square$

\textbf{Property 2 (Abstention correctness):} If any gate $g_i(x) = R^*$, then $r_{\text{final}}(x) = R^*$. \textit{Proof:} Lines 4--8 return $R^*$ immediately when detected in gate outputs. $\square$

\textbf{Property 3 (No critical dilution):} If any gate $g_i(x) \in \{R1, R2\}$ (critical/high-risk), then $r_{\text{final}}(x) \in \{R^*, R1, R2\}$ (cannot be downgraded to moderate/low/minimal). \textit{Proof:} Since $R1, R2 \sqsubseteq R3, R4, R5$ on lattice, minimum selection cannot produce less conservative tier than any input containing R1 or R2. $\square$

These properties provide formal guarantees that individual knowledge module warnings cannot be suppressed through statistical combination—addressing the ensemble averaging failure mode.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/safety_performance.pdf}
\caption{Tier-specific safety performance analysis validating formal safety properties. (a) Sensitivity across clinical triage categories demonstrates perfect 100\% critical detection (R1-R2 combined), with 100\% R1 detection and 89.8\% R2 detection. (b) Precision across categories shows perfect 100\% safe discharge precision (R5), ensuring zero false safe discharges. (c) Balanced F1-score achieves 64.6\% macro average while maintaining zero false negatives on critical cases. The performance pattern validates Properties 1--3: conservative preservation (all gates contribute to final tier), abstention correctness (no unsafe deferrals), and no critical dilution (R1-R2 cases never downgraded to R3-R5).}
\label{fig:safety_performance}
\end{figure}

\subsection{Synthetic Data Generation with Counterfactual Reasoning}

We generated 6,400 synthetic vertigo cases combining counterfactual reasoning with non-negative matrix factorization (NMF) to ensure clinically plausible symptom co-occurrence patterns. This approach enables controlled testing of safety properties while avoiding patient privacy concerns and institutional review board delays associated with real clinical data.

\textbf{Counterfactual generation:} Starting from 320 expert-annotated real ED cases (collected under IRB approval, used only as templates), generate synthetic variants by systematically perturbing individual features while preserving medical plausibility. For example, changing ``age 45 years'' to ``age 72 years'' requires adding age-appropriate cardiovascular risk factors (hypertension, atrial fibrillation, diabetes) to maintain realistic presentation. This creates decision boundary exploration: synthetic cases straddle tier transitions, testing gate behavior on borderline presentations.

\textbf{NMF symptom consistency:} Non-negative matrix factorization with rank-15 latent factors enforces realistic symptom co-occurrence learned from real ED data statistics. Factor analysis reveals clinical patterns: Factor 1 (peripheral vestibular) loads heavily on positional triggers and brief duration; Factor 2 (central vascular) emphasizes cardiovascular risk factors and focal deficits; Factor 3 (incomplete data) represents missing field patterns. Generated cases project onto factor space; violations (implausible combinations like ``20-year-old with extensive cardiovascular disease history'') trigger rejection and regeneration.

\textbf{Tier distribution:} Final 6,400-case dataset reflects realistic emergency distributions: R1 critical (5.9\%, n=378), R2 high-risk (15.8\%, n=1,009), R3 moderate (37.3\%, n=2,387), R4 low-risk (27.6\%, n=1,766), R5 minimal (13.4\%, n=860). This 40:1 ratio of benign to life-threatening matches epidemiological studies \cite{newman2008spectrum}, creating challenging imbalanced classification scenario where statistical methods naturally bias toward prevalent benign class.

\textbf{Feature coverage:} Each synthetic case comprises 52 features spanning demographics (age, sex), vital signs (blood pressure, heart rate, respiratory rate, oxygen saturation, temperature), symptom characteristics (onset pattern, duration, triggers, exacerbating/relieving factors, severity), neurological examination (HINTS test components, gait, coordination, cranial nerves II-XII), medical history (prior stroke/TIA, cardiovascular disease, diabetes, atrial fibrillation, medications), and temporal evolution (symptom progression trajectory, time from onset).

\textbf{Validation:} Stratified sampling partitions data preserving tier distributions: training (n=4,800, 75\%), validation (n=798, 12.5\%), test (n=804, 12.5\%). Independent expert physician review (2 emergency physicians, 1 neurologist) assessed 100 randomly sampled synthetic cases for clinical plausibility: 94\% rated ``realistic,'' 4\% ``possible but uncommon,'' 2\% ``implausible'' (rejected and regenerated). Inter-rater agreement (Fleiss' $\kappa = 0.79$) indicates substantial consensus on quality.

Computational verification confirms zero violations of six safety properties across all 6,400 cases (training + validation + test), validating both synthetic data integrity and gate logic correctness under controlled testing conditions.

\section{Results}
\label{sec:results}

\subsection{Overall Expert System Performance}

We evaluated \safegate{} on 804 held-out test cases that the system had never seen during development. Table \ref{tab:overall_performance} summarizes the key performance metrics. The results show that our conservative architecture achieved its primary design goal: perfect detection of all critical cases. Every single one of the 175 high-severity cases (R1 and R2 tiers) was correctly identified, yielding 100\% sensitivity with zero false negatives. The system also maintained perfect precision for safe discharge recommendations—not a single patient who should have been discharged was incorrectly flagged for escalation.

\begin{table}[htbp]
\centering
\caption{Overall performance metrics for \safegate{} on 804 held-out test cases. The system achieved its primary safety objective with 100\% sensitivity for critical cases (R1-R2) and 100\% precision for safe discharge recommendations (R5), though overall accuracy of 59.6\% and discharge specificity of 57.4\% reflect the deliberate over-triage design choice.}
\label{tab:overall_performance}
\small
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{Details} \\
\midrule
Critical sensitivity (R1--R2) & \textbf{100.0\%} & 175/175 cases \\
Discharge specificity (R5) & 57.4\% & 62/108 cases \\
False negative rate & \textbf{0.0\%} & Zero missed \\
Overall accuracy & 59.6\% & 479/804 correct \\
Macro F1-score & 64.6\% & Across all tiers \\
Abstention rate & 0.0\% & Current calibration \\
\bottomrule
\end{tabular}
\end{table}

However, these safety guarantees came with a cost. Overall accuracy reached only 59.6\%, substantially lower than what a purely accuracy-optimized model might achieve. Discharge specificity stood at 57.4\%—meaning that among patients who truly had minimal risk, the system correctly identified only about 57% as safe for discharge, while the remaining 43% were unnecessarily escalated to higher tiers. This reflects our deliberately conservative design. The current configuration shows 0\% abstention rate due to threshold recalibration during development; clinical deployment would likely require restoring a 12--15\% deferral rate to maintain appropriate human oversight for genuinely ambiguous cases.

\subsection{Per-Tier Performance and Error Patterns}

Breaking down performance by individual risk tier reveals where the conservative architecture makes its trade-offs. Table \ref{tab:perclass_performance} shows precision, recall, and F1-scores for each tier. The R1 critical tier achieved perfect 100\% recall—we didn't miss a single critical case among the 48 in our test set. Precision for R1 stood at 78.7%, meaning 13 cases that weren't actually R1 got escalated to that tier. But these "errors" were actually conservative misclassifications from R2 (high-risk cases escalated to critical), which represents acceptable over-triage rather than dangerous under-triage.

\begin{table}[htbp]
\centering
\caption{Performance metrics broken down by risk tier. Perfect R1 recall (100\%) and R5 precision (100\%) demonstrate the safety-first design. Low R4 recall (28.4\%) reveals where conservative escalation primarily occurs—borderline low-risk cases get bumped to R3 moderate.}
\label{tab:perclass_performance}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Tier} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Support} \\
\midrule
R1 (Critical) & 78.7\% & \textbf{100.0\%} & 88.1\% & 48 \\
R2 (High-risk) & 49.1\% & 89.8\% & 63.5\% & 127 \\
R3 (Moderate) & 54.2\% & 64.2\% & 58.8\% & 299 \\
R4 (Low-risk) & 66.3\% & 28.4\% & 39.8\% & 222 \\
R5 (Minimal) & \textbf{100.0\%} & 57.4\% & 72.9\% & 108 \\
\midrule
\textbf{Macro average} & 69.7\% & 67.9\% & \textbf{64.6\%} & 804 \\
\bottomrule
\end{tabular}
\end{table}

The R2 high-risk tier showed 89.8\% recall—we caught 114 out of 127 cases, with the 13 missed cases actually escalated to R1 rather than downgraded, maintaining the conservative direction. R3 moderate demonstrated more balanced performance at 64.2\% recall and 54.2\% precision, reflecting the genuine ambiguity in this middle tier where clinical judgment varies even among expert physicians.

The R4 low-risk tier revealed the clearest signature of our conservative architecture. Recall dropped to just 28.4%—only 63 out of 222 R4 cases were correctly classified, while a substantial 159 cases (71.6\%) got escalated to R3. This represents the primary source of over-triage in our system. When the cardiovascular risk module (G2) accumulates moderate risk factors, or when the uncertainty module (G5) detects elevated prediction variance, borderline low-risk cases get bumped up to moderate as a precautionary measure.

Finally, R5 minimal-risk showed perfect 100\% precision—zero false safe discharges—but only 57.4\% recall. Among the 108 truly minimal-risk cases, 62 were correctly identified for safe discharge while 46 got escalated to R3. Critically, none of these escalations went in a dangerous direction; the system erred on the side of caution.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/per_class_metrics.pdf}
\caption{Per-class performance metrics across risk tiers. (a) Precision per tier shows R5 achieving perfect 100\% precision with zero false safe discharges. (b) Recall per tier demonstrates R1 achieving perfect 100\% detection with zero false negatives. (c) F1-score per tier shows balanced performance with macro F1 of 64.6\%. The pattern reveals conservative architecture characteristics: high recall for critical tiers (R1-R2) at the cost of lower recall for safe discharge (R5), reflecting the deliberate over-triage design.}
\label{fig:per_class_metrics}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{figures/confusion_matrix.png}
\caption{Confusion matrix for \safegate{} predictions on 804 test cases. The matrix reveals the conservative escalation pattern: most misclassifications occur above the diagonal (predicting higher risk than actual), with minimal dangerous under-triage below the diagonal. Zero critical cases (R1-R2) were misclassified as safe discharge (R5), validating the safety-first architecture. The primary over-triage occurs at R4→R3 (159 cases) and R5→R3 (46 cases), representing the conservative bias trade-off.}
\label{fig:confusion_matrix}
\end{figure}

\subsection{Baseline Comparison: Demonstrating Ensemble Failure}

The most important validation comes from comparing \safegate{} against alternative approaches on the identical test set. Table \ref{tab:baseline_comparison} shows results for four baseline methods. The Emergency Severity Index (ESI) guidelines represent maximum conservatism—referring essentially all acute vertigo patients for immediate evaluation—which achieves 100\% sensitivity but 0\% specificity, an obviously impractical extreme.

\begin{table}[htbp]
\centering
\caption{Comparison against baseline methods on 804 test cases. Ensemble averaging exhibits catastrophic failure (71.4\% sensitivity, missing 50 critical cases). \safegate{} matches perfect sensitivity while maintaining practical specificity and avoiding excessive abstention.}
\label{tab:baseline_comparison}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{R1-R2 Sensitivity} & \textbf{R5 Specificity} & \textbf{Abstention} \\
\midrule
ESI Guidelines & 100.0\% & 0.0\% & 0.0\% \\
Single XGBoost & 100.0\% & 83.3\% & 0.0\% \\
Ensemble Averaging & \textbf{71.4\%} & 0.0\% & 0.0\% \\
Confidence Threshold & 100.0\% & 0.0\% & 99.1\% \\
\midrule
\textbf{\safegate{} (Ours)} & \textbf{100.0\%} & 57.4\% & 0.0\% \\
\bottomrule
\end{tabular}
\end{table}

A single gradient-boosted tree (XGBoost) trained on the full feature set actually performed remarkably well, achieving both 100\% sensitivity and 83.3\% specificity—better than our system on specificity. This likely reflects that our synthetic data, despite careful construction, remains simpler than real emergency department presentations. The strong performance of a single model suggests the synthetic test cases may have clearer decision boundaries than we'd encounter clinically.

But here's the crucial finding: when we took five diverse models (XGBoost, Random Forest, Logistic Regression, Neural Network, and SVM) and combined them through standard ensemble averaging, sensitivity collapsed catastrophically to 71.4\%. This ensemble missed 50 out of 175 critical cases—a 28.6\% false negative rate that would be completely unacceptable in clinical practice. Examining these failures revealed exactly the mechanism we predicted: when one model correctly identified a critical case but got outvoted by four others predicting moderate risk, the averaged probability fell to moderate, losing the safety signal.

The confidence thresholding baseline achieved 100\% sensitivity by abstaining on 797 out of 804 cases (99.1% abstention rate). While theoretically safe, this approach provides almost no practical value—it essentially defers every decision back to clinicians.

\safegate{} matched the perfect 100\% sensitivity of the best baselines while maintaining a practical 57.4\% specificity and 0\% abstention in current calibration. The lower specificity compared to single XGBoost represents an architectural choice—conservative merging favors sensitivity over specificity through systematic escalation of borderline cases. Importantly, we avoid the catastrophic ensemble failure and the impractical abstention rates of alternative approaches.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/baseline_comparison.pdf}
\caption{Baseline method comparison across three key safety metrics. (a) Sensitivity for critical case detection (R1-R2) shows \safegate{} achieves perfect 100\% sensitivity, matching ESI Guidelines and Single XGBoost, while standard Ensemble Averaging catastrophically fails at 71.4\%, missing 50 out of 175 critical cases. (b) Specificity for safe discharge (R5) demonstrates the conservative over-triage trade-off, with \safegate{} at 57.4\% compared to Single XGBoost at 83.3\%. (c) Abstention rate (R* tier) shows \safegate{} maintains practical 0\% abstention, avoiding the Confidence Threshold method's impractical 99.1\% deferral rate. The results validate that conservative MIN selection eliminates ensemble averaging's safety signal dilution.}
\label{fig:baseline_comparison}
\end{figure}

\subsection{Distribution Shifts and Individual Module Contributions}

Examining how \safegate{}'s predictions shifted relative to ground truth reveals the conservative bias in action. Compared to the true distribution, our system's predictions showed R2 (high-risk) nearly doubling from 15.8\% to 32.2\%—an increase of 132 cases. Simultaneously, R4 (low-risk) halved from 27.6\% to 12.4\%, losing 122 cases, and R5 (minimal) halved from 13.4\% to 7.7\%, losing 46 cases. This rightward shift along the risk spectrum indicates that 171 cases (21.3\% of the total) were escalated at least one tier higher than their true classification.

Looking at individual gate contributions helps explain these patterns. Gate G1 (critical red flags) operated as designed—detecting every case with life-threatening features and contributing the perfect R1 recall. Gate G2 (cardiovascular risk scoring) was the primary driver of R4-to-R3 escalations, triggering when accumulated risk factors crossed thresholds. Gate G3 (data quality) successfully enforced completeness requirements but produced minimal abstentions in our carefully curated synthetic dataset; real-world deployment with incomplete documentation would likely see higher G3 abstention rates. Gate G4 (syndrome matching) correctly identified well-characterized benign patterns, enabling the 57.4% of safe discharges we achieved. Gate G5 (uncertainty quantification) escalated cases when prediction variance exceeded thresholds, contributing to R4-to-R3 and R5-to-R3 escalations. Gate G6 (temporal analysis) enforced appropriate urgency for acute presentations within the stroke window.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figures/risk_distribution.png}
\caption{Risk tier distribution comparison between ground truth and \safegate{} predictions. The conservative architecture produces systematic rightward shift: R2 (high-risk) nearly doubles from 15.8\% to 32.2\% (+132 cases), while R4 (low-risk) halves from 27.6\% to 12.4\% (-122 cases) and R5 (minimal) halves from 13.4\% to 7.7\% (-46 cases). This pattern represents 171 cases (21.3\%) escalated at least one tier higher than ground truth, implementing the conservative bias that guarantees zero false negatives on critical cases.}
\label{fig:risk_distribution}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figures/support_distribution.png}
\caption{Class support distribution across 804 test cases showing the realistic class imbalance. Critical cases (R1) comprise only 6.0\% (48 cases) and high-risk (R2) 15.8\% (127 cases), totaling 21.8\% for the combined R1-R2 critical tier. Moderate cases (R3) represent the largest group at 37.2\% (299 cases), with low-risk (R4) at 27.6\% (222 cases) and minimal-risk safe discharge (R5) at 13.4\% (108 cases). This severe imbalance (critical-to-benign ratio approximately 1:3.6) demonstrates why standard ensemble averaging fails—statistical optimization naturally gravitates toward the majority benign classes, diluting critical case detection signals.}
\label{fig:support_distribution}
\end{figure}

\section{Discussion}
\label{sec:discussion}

These results validate three core hypotheses about conservative knowledge integration for safety-critical expert systems, while also revealing important limitations and requirements for clinical translation.

\subsection{Primary Findings and Implications}

\textbf{First}, conservative merging via lattice minimum selection successfully eliminates the ensemble averaging failure mode. Our direct comparison demonstrated a stark difference: standard averaging achieved only 71.4\% sensitivity (missing 50 of 175 critical cases), while conservative merging achieved 100\% sensitivity with zero misses. This 28.6% false negative rate for averaging would be catastrophic in clinical practice. The failure mechanism we observed matched our theoretical prediction—when one model correctly flags a critical case but gets outvoted by others, averaging dilutes the safety signal. By selecting the minimum on our risk lattice instead, we ensure that conservative warnings always propagate through to the final output. This architectural principle should generalize beyond medical triage to any safety-critical domain where missing dangerous cases causes more harm than raising false alarms.

\textbf{Second}, systematic over-triage represents a necessary trade-off for achieving perfect critical detection. Our system escalated 171 out of 804 cases (21.3\%) beyond their true severity, with most escalations occurring from R4 low-risk to R3 moderate (159 cases, 71.6\%) and R5 minimal to R3 (46 cases, 42.6\%). This resulted in 57.4\% discharge specificity compared to 83.3\% for the single XGBoost baseline. Whether this trade-off is clinically acceptable requires expert physician input. Emergency physicians might tolerate a 21\% over-triage rate if it genuinely eliminates missed strokes. Alternatively, they might find that unnecessary moderate-risk escalations create unsustainable workload and resource utilization. This judgment depends on local capacity, patient volume, staffing levels, and institutional risk tolerance—factors that vary across emergency departments.

\textbf{Third}, hybrid symbolic-statistical architecture provides complementary strengths. Rule-based gates (G1 critical red flags, G4 syndrome matching) encode established clinical knowledge with perfect transparency—physicians can directly inspect and validate the logic. Statistical gates (G2 cardiovascular risk, G5 uncertainty quantification) capture probabilistic patterns and handle the continuous nature of risk assessment. This combination addresses limitations of both traditional expert systems (brittleness) and pure machine learning (black-box nature). The conservative merging framework ensures that hard safety constraints encoded in rules cannot be overridden by statistical predictions that might have learned dangerous shortcuts.

\subsection{Limitations and Validation Requirements}

Five critical limitations constrain interpretation and clinical applicability of these results. \textbf{First}, we validated exclusively on synthetic data. Despite careful construction using counterfactual reasoning and non-negative matrix factorization to ensure realistic symptom co-occurrence patterns, synthetic cases cannot capture the full complexity of actual emergency presentations. Real patients present with atypical combinations of symptoms, incomplete historical information, measurement errors, and documentation gaps that synthetic generation methods struggle to replicate. The strong performance of our single XGBoost baseline (83.3\% specificity, 100\% sensitivity) likely indicates that synthetic test cases have clearer decision boundaries than we would encounter in real emergency departments.

\textbf{Second}, no expert physicians have reviewed or validated the encoded clinical knowledge. All gate logic, red flag definitions, risk scoring weights, syndrome matching criteria, and safety property specifications derive from our literature review rather than active collaboration with practicing clinicians. We don't know how well our formalization matches actual emergency physician reasoning or whether we've introduced errors in translating clinical guidelines to computational logic. Inter-rater agreement between \safegate{} and experienced emergency physicians remains unknown and requires systematic evaluation.

\textbf{Third}, the system addresses only one specific clinical domain—acute vertigo and dizziness. The conservative integration architecture should generalize to other emergency presentations, but the specific gates, features, and thresholds do not. Chest pain triage would require different safety dimensions; abdominal pain would need yet another set of modules. Each clinical domain demands its own knowledge engineering effort, including identification of relevant safety dimensions, encoding of domain-specific clinical rules, and calibration of statistical components. The transferability of our approach across domains remains an empirical question.

\textbf{Fourth}, abstention mechanisms need prospective calibration. Our current configuration shows 0\% abstention due to threshold adjustments during development. Clinical deployment would require restoring the G3 data quality module and G5 uncertainty module abstention functions to defer genuinely ambiguous cases back to human clinicians. The appropriate abstention rate depends on local workflow constraints—what percentage of human review can the emergency department accommodate without overwhelming providers? This requires prospective observational deployment to identify suitable thresholds balancing automation benefits against review burden.

\textbf{Fifth}, we have no patient outcome data. All our metrics measure classification performance on synthetic labels. We cannot say whether deploying \safegate{} would actually improve the outcomes that matter: stroke detection rates, time to thrombolysis, 90-day functional outcomes, mortality, cost-effectiveness, or clinician workload. These patient-centered outcomes require prospective randomized controlled trials comparing AI-assisted versus conventional triage.

\subsection{Path to Clinical Validation}

Responsible clinical deployment would require a staged validation pathway. \textbf{Phase 0} (Expert Review) should engage emergency physicians and neurologists to audit gate logic through structured case review, targeting Cohen's kappa above 0.75 for inter-rater agreement between physicians and the system. Physicians should verify red flag definitions against current guidelines, validate syndrome matching criteria, and assess whether risk scoring weights align with clinical practice. This qualitative validation must precede any patient data exposure.

\textbf{Phase 1} (Retrospective Validation) would evaluate the system on de-identified historical emergency department records, measuring sensitivity, specificity, and calibration on actual case mix with real documentation patterns and missing data. This phase would reveal how synthetic-to-real generalization affects performance and whether our 100\% sensitivity holds on genuine presentations. It would also identify specific failure modes—categories of cases the system handles poorly—to guide refinement.

\textbf{Phase 2} (Prospective Observational) would deploy the system in non-interventional mode, showing recommendations to clinicians while tracking how often they agree, override, or modify the system's assessment. This measures clinical utility and trust. High override rates would indicate poor calibration to local practice patterns. Low system utilization would suggest workflow integration problems. This phase also allows prospective abstention threshold calibration based on observed review capacity.

\textbf{Phase 3} (Randomized Controlled Trial) would test whether AI-assisted triage actually improves patient outcomes through cluster-randomized design comparing shifts or departments using \safegate{} versus conventional triage. Primary outcomes should include stroke detection rates within the thrombolysis window, door-to-needle times for eligible patients, and 90-day modified Rankin scores measuring functional disability. Secondary outcomes should assess resource utilization (imaging rates, admission rates), cost-effectiveness, and clinician workload. Only positive RCT results would justify widespread deployment.

\subsection{Architectural Generalizability}

The conservative knowledge integration principle should apply beyond medical triage. Any safety-critical domain where false negatives cause substantially more harm than false positives could benefit from lattice-based minimum selection instead of ensemble averaging. Potential applications include financial fraud detection (missing a fraud case costs more than investigating a false alarm), industrial safety monitoring (missing equipment failure causes more harm than unnecessary maintenance), security threat detection (missing an attack costs more than false alerts), and autonomous vehicle emergency decision-making (missing a pedestrian is far worse than unnecessary braking).

The key requirement is that the domain admits a natural risk ordering where conservative precedence makes sense. For purely symmetric error costs, ensemble averaging's accuracy optimization remains appropriate. But many real-world applications exhibit asymmetric loss functions where conservative bias aligns with operational priorities. For these domains, our framework provides both architectural patterns (parallel knowledge modules with orthogonal coverage, lattice-based merging) and verification methodology (formal safety properties, computational checking).

\section{Conclusion}
\label{sec:conclusion}

We developed and computationally validated \safegate{}, a hybrid expert system combining symbolic clinical knowledge with statistical learning through conservative lattice-based integration. Testing on 804 synthetic vertigo cases demonstrated perfect critical detection (100\% sensitivity capturing all 175 high-severity cases), flawless safe discharge precision (100\%, zero false safe discharges), and 59.6\% overall accuracy reflecting deliberate conservative bias. The system exhibits systematic over-triage (171 escalated cases, 21.3\%), trading specificity (57.4\%) for guaranteed critical detection—a design choice that aligns with emergency medicine's "first, do no harm" principle.

Direct comparison against baseline methods validated our core architectural hypothesis. Standard ensemble averaging exhibited catastrophic failure, achieving only 71.4\% sensitivity and missing 50 out of 175 critical cases. This dramatic difference demonstrates that averaging dilutes safety signals when conservative models get outvoted—exactly the failure mode our lattice-based minimum selection eliminates. By always selecting the most cautious assessment across parallel knowledge modules, we ensure that safety warnings cannot be suppressed by statistical combination.

This work contributes to expert systems literature in four ways. \textbf{First}, we formalize conservative knowledge integration that provably eliminates averaging-induced safety failures through lattice minimum selection rather than statistical combination. \textbf{Second}, we demonstrate effective hybrid architecture combining symbolic rules (G1, G3, G4, G6) with statistical learning (G2, G5), providing both interpretability and adaptability. \textbf{Third}, we develop a safety property framework with computational verification, establishing six mathematical properties and achieving zero violations across 6,400 cases. \textbf{Fourth}, we empirically validate the ensemble averaging failure hypothesis through controlled comparison.

However, this computational validation using synthetic data represents only an initial feasibility study. Clinical deployment requires systematic expert validation: emergency physicians and neurologists must audit the encoded clinical knowledge to verify correctness and alignment with guidelines. Retrospective evaluation on de-identified patient records must assess real-world performance. Prospective observational deployment must measure clinician-system concordance and calibrate abstention thresholds. Ultimately, randomized controlled trials must demonstrate that the system actually improves patient outcomes—stroke detection rates, time to treatment, functional outcomes at 90 days—not just synthetic classification metrics.

The architecture provides a generalizable template for safety-critical expert systems in domains where missing dangerous cases causes far more harm than unnecessary escalation. By rejecting ensemble averaging in favor of conservative integration, incorporating formal safety verification, and maintaining interpretable reasoning chains, we demonstrate how modern AI capabilities can coexist with the safety guarantees that high-stakes applications demand.

Complete implementation source code, synthetic datasets, and reproducibility protocols are publicly available at \url{https://github.com/ChatchaiTritham/SAFE-Gate} to enable independent validation, expert review, and collaborative refinement.

%% Ethics and Data Availability
\section*{Ethics Approval and Data Availability}

\textbf{Ethics:} This computational study uses entirely synthetic clinical data generated through counterfactual reasoning. No human subjects or patient data were involved. Institutional review board approval was not required.

\textbf{Data availability:} Complete source code, synthetic datasets, reproducibility protocols, and gate logic specifications are publicly available at \url{https://github.com/ChatchaiTritham/SAFE-Gate} under MIT license to enable expert review, independent validation, and collaborative improvement.

\textbf{Author contributions:} C.T. designed the system architecture, implemented the gates, conducted experiments, and drafted the manuscript. C.S.N. supervised the research, provided domain expertise, and critically revised the manuscript. Both authors approved the final version.

\textbf{Conflict of interest:} The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

%% Acknowledgments
\section*{Acknowledgments}

We thank emergency medicine, otolaryngology, and neurology clinicians at Pranangklao Hospital for domain expertise informing gate logic design. This computational validation requires expert physician review before any clinical application.

%% References
\bibliographystyle{elsarticle-num}
\bibliography{references}

\end{document}
